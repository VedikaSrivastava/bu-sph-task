{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import scipy.io\n",
    "from pretrain_gnns.bio.model import GNN\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch_geometric.data import Batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adapt the BioDataset class to load .mat files\n",
    "# class MatBioDataset(BioDataset):\n",
    "#     def __init__(self, mat_file, transform=None):\n",
    "#         self.data = scipy.io.loadmat(mat_file)\n",
    "#         self.graphs = []\n",
    "#         self.labels = []\n",
    "\n",
    "#         x = torch.tensor(self.data['attrb'].todense(), dtype=torch.float32)\n",
    "#         edge_index = torch.tensor(self.data['network'].nonzero(), dtype=torch.long)\n",
    "#         y = torch.tensor(self.data['group'].argmax(axis=1).squeeze(), dtype=torch.long)\n",
    "#         edge_attr = torch.ones(edge_index.shape[1], 9)\n",
    "        \n",
    "#         self.graphs.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr))\n",
    "#         self.labels.append(y)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.graphs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.graphs[idx], self.labels[idx]\n",
    "\n",
    "# Simple Dataset class\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mat_file):\n",
    "        self.data = scipy.io.loadmat(mat_file)\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        x = torch.tensor(self.data['attrb'].todense(), dtype=torch.float32)\n",
    "        edge_index = torch.tensor(self.data['network'].nonzero(), dtype=torch.long)\n",
    "        y = torch.tensor(self.data['group'].argmax(axis=1).squeeze(), dtype=torch.long)\n",
    "        edge_attr = torch.ones(edge_index.shape[1], 9)\n",
    "\n",
    "        self.graphs.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr))\n",
    "        self.labels.append(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return Batch.from_data_list(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akalps\\AppData\\Local\\Temp\\ipykernel_4804\\652844516.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  edge_index = torch.tensor(self.data['network'].nonzero(), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_dataset = SimpleDataset('acmv9.mat')\n",
    "test_dataset = SimpleDataset('citationv1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = GNN(num_layer=5, emb_dim=300, gnn_type='gin')\n",
    "model.load_state_dict(torch.load('pretrain_gnns/bio/model_gin/supervised.pth'), strict=False)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tupleBatch' object has no attribute 'stores_as'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      5\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(batch)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m collate(\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m     99\u001b[0m         data_list\u001b[38;5;241m=\u001b[39mdata_list,\n\u001b[0;32m    100\u001b[0m         increment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    101\u001b[0m         add_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_list[\u001b[38;5;241m0\u001b[39m], Batch),\n\u001b[0;32m    102\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39mfollow_batch,\n\u001b[0;32m    103\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39mexclude_keys,\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\collate.py:61\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     58\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Create empty stores:\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m out\u001b[38;5;241m.\u001b[39mstores_as(data_list[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     63\u001b[0m follow_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(follow_batch \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[0;32m     64\u001b[0m exclude_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude_keys \u001b[38;5;129;01mor\u001b[39;00m [])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tupleBatch' object has no attribute 'stores_as'"
     ]
    }
   ],
   "source": [
    "# Fine-tuning loop\n",
    "model.train()\n",
    "for epoch in range(25):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            output = model(batch[0].x.to(device), batch[0].edge_index.to(device), batch[0].edge_attr.to(device))\n",
    "            loss = criterion(output, batch[1].to(device))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        with autocast():\n",
    "            output = model(batch[0].x.to(device), batch[0].edge_index.to(device), batch[0].edge_attr.to(device))\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_labels.append(batch[1].cpu())\n",
    "\n",
    "all_predictions = torch.cat(all_predictions)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "micro_f1 = f1_score(all_labels, all_predictions, average='micro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Micro F1 Score: {micro_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not Data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m edge_attr_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(citation_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetwork\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtodense(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Adjust if needed\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Create PyTorch Geometric Data objects\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m train_data \u001b[38;5;241m=\u001b[39m [BioDataset(Data(x\u001b[38;5;241m=\u001b[39mX_train, edge_index\u001b[38;5;241m=\u001b[39medge_index_train, edge_attr\u001b[38;5;241m=\u001b[39medge_attr_train, y\u001b[38;5;241m=\u001b[39my_train, data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupervised\u001b[39m\u001b[38;5;124m'\u001b[39m), data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupervised\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     28\u001b[0m test_data \u001b[38;5;241m=\u001b[39m [BioDataset(Data(x\u001b[38;5;241m=\u001b[39mX_test, edge_index\u001b[38;5;241m=\u001b[39medge_index_test, edge_attr\u001b[38;5;241m=\u001b[39medge_attr_test, y\u001b[38;5;241m=\u001b[39my_test, data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupervised\u001b[39m\u001b[38;5;124m'\u001b[39m), data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupervised\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Step 2: Create DataLoader\u001b[39;00m\n",
      "File \u001b[1;32mf:\\bu-sph-task\\pretrain_gnns\\bio\\loader.py:165\u001b[0m, in \u001b[0;36mBioDataset.__init__\u001b[1;34m(self, root, data_type, empty, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m root\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_type \u001b[38;5;241m=\u001b[39m data_type\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28msuper\u001b[39m(BioDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform, pre_transform, pre_filter)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m empty:\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_paths[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:81\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     74\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     force_reload: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform, pre_transform, pre_filter, log,\n\u001b[0;32m     82\u001b[0m                      force_reload)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data: Optional[BaseData] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:112\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_reload \u001b[38;5;241m=\u001b[39m force_reload\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_download:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process()\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:225\u001b[0m, in \u001b[0;36mDataset._download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m files_exist(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_paths):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     fs\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:205\u001b[0m, in \u001b[0;36mDataset.raw_paths\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, Callable):\n\u001b[0;32m    204\u001b[0m     files \u001b[38;5;241m=\u001b[39m files()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m to_list(files)]\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:205\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, Callable):\n\u001b[0;32m    204\u001b[0m     files \u001b[38;5;241m=\u001b[39m files()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m to_list(files)]\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:122\u001b[0m, in \u001b[0;36mDataset.raw_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_dir\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m<frozen ntpath>:108\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not Data"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import scipy.io\n",
    "from torch.utils.data import DataLoader\n",
    "from pretrain_gnns.bio.loader import BioDataset\n",
    "from pretrain_gnns.bio.dataloader import DataLoaderFinetune\n",
    "from pretrain_gnns.bio.model import GNN\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "acm_data = scipy.io.loadmat('acmv9.mat')\n",
    "citation_data = scipy.io.loadmat('citationv1.mat')\n",
    "\n",
    "X_train = torch.tensor(acm_data['attrb'].todense(), dtype=torch.float32)\n",
    "y_train = torch.tensor(acm_data['group'].argmax(axis=1).squeeze(), dtype=torch.long)\n",
    "edge_index_train = torch.tensor(acm_data['network'].nonzero(), dtype=torch.long)\n",
    "edge_attr_train = torch.tensor(acm_data['network'].todense(), dtype=torch.float32)  # Adjust if needed\n",
    "\n",
    "X_test = torch.tensor(citation_data['attrb'].todense(), dtype=torch.float32)\n",
    "y_test = torch.tensor(citation_data['group'].argmax(axis=1).squeeze(), dtype=torch.long)\n",
    "edge_index_test = torch.tensor(citation_data['network'].nonzero(), dtype=torch.long)\n",
    "edge_attr_test = torch.tensor(citation_data['network'].todense(), dtype=torch.float32)  # Adjust if needed\n",
    "\n",
    "# Create PyTorch Geometric Data objects\n",
    "train_data = [BioDataset(Data(x=X_train, edge_index=edge_index_train, edge_attr=edge_attr_train, y=y_train, data_type='supervised'), data_type='supervised')]\n",
    "test_data = [BioDataset(Data(x=X_test, edge_index=edge_index_test, edge_attr=edge_attr_test, y=y_test, data_type='supervised'), data_type='supervised')]\n",
    "\n",
    "# Step 2: Create DataLoader\n",
    "train_loader = DataLoaderFinetune(train_data, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoaderFinetune(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "# Step 3: Load pretrained model\n",
    "model = GNN(num_layer=5, emb_dim=300, gnn_type='gin')  # Adjust num_layer and emb_dim as per pretrained model\n",
    "checkpoint = torch.load('pretrain_gnns/bio/model_gin/supervised.pth')\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Step 4: Set up optimizer, loss function, and scaler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Step 5: Training loop\n",
    "model.train()\n",
    "for epoch in range(25):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            # Move batch data to the device\n",
    "            batch = batch.to(device)\n",
    "            output = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            loss = criterion(output, batch.y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Step 6: Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        output = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        all_preds.append(predictions.cpu().numpy())\n",
    "        all_labels.append(batch.y.cpu().numpy())\n",
    "\n",
    "# Step 7: Calculate metrics\n",
    "accuracy = accuracy_score(np.concatenate(all_labels), np.concatenate(all_preds))\n",
    "micro_f1 = f1_score(np.concatenate(all_labels), np.concatenate(all_preds), average='micro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Micro F1 Score: {micro_f1:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
